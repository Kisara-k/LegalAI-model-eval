{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b240d006",
   "metadata": {},
   "source": [
    "# Reranker Evaluation\n",
    "\n",
    "This notebook applies cross-encoder reranking to improve initial retrieval results.\n",
    "\n",
    "Reranking is applied to:\n",
    "- BM25 results (content and metadata)\n",
    "- FAISS results (selected models)\n",
    "\n",
    "Reranker: `cross-encoder/ms-marco-MiniLM-L-6-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ee80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_loader import load_data, prepare_data, get_documents_by_field\n",
    "from src.queries import get_all_queries\n",
    "from src.reranker import Reranker, evaluate_reranking, save_reranking_results\n",
    "from src.evaluation import print_query_results, calculate_overlap\n",
    "from src.config import RESULTS_DIR, TOP_K\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04771fdd",
   "metadata": {},
   "source": [
    "## 1. Load Data and Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05127e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_data()\n",
    "df = prepare_data(df)\n",
    "\n",
    "# Get document lists\n",
    "documents_content = get_documents_by_field(df, 'content')\n",
    "documents_metadata = get_documents_by_field(df, 'metadata')\n",
    "\n",
    "# Get queries\n",
    "queries = get_all_queries()\n",
    "\n",
    "print(f\"Loaded {len(df)} documents and {len(queries)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous retrieval results\n",
    "def load_results(filename):\n",
    "    \"\"\"Load results from JSON file.\"\"\"\n",
    "    filepath = RESULTS_DIR / filename\n",
    "    if filepath.exists():\n",
    "        with open(filepath, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(f\"Warning: {filename} not found\")\n",
    "        return None\n",
    "\n",
    "# Load BM25 results\n",
    "bm25_content = load_results(\"bm25_content_results.json\")\n",
    "bm25_metadata = load_results(\"bm25_metadata_results.json\")\n",
    "\n",
    "# Load FAISS results (Legal-BERT as example)\n",
    "faiss_legalbert_content = load_results(\"faiss_legal-bert_content_results.json\")\n",
    "faiss_legalbert_metadata = load_results(\"faiss_legal-bert_metadata_results.json\")\n",
    "\n",
    "# Load FAISS results (BGE-Large as example)\n",
    "faiss_bge_content = load_results(\"faiss_bge-large_content_results.json\")\n",
    "faiss_bge_metadata = load_results(\"faiss_bge-large_metadata_results.json\")\n",
    "\n",
    "print(\"\\nLoaded results:\")\n",
    "for name, result in [\n",
    "    (\"BM25 Content\", bm25_content),\n",
    "    (\"BM25 Metadata\", bm25_metadata),\n",
    "    (\"FAISS Legal-BERT Content\", faiss_legalbert_content),\n",
    "    (\"FAISS Legal-BERT Metadata\", faiss_legalbert_metadata),\n",
    "    (\"FAISS BGE-Large Content\", faiss_bge_content),\n",
    "    (\"FAISS BGE-Large Metadata\", faiss_bge_metadata),\n",
    "]:\n",
    "    status = \"✓\" if result is not None else \"✗\"\n",
    "    print(f\"{status} {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f50c1",
   "metadata": {},
   "source": [
    "## 2. Initialize Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the reranker (will be reused)\n",
    "reranker = Reranker()\n",
    "print(\"✓ Reranker initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db455f",
   "metadata": {},
   "source": [
    "## 3. Rerank BM25 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a389459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank BM25 content results\n",
    "if bm25_content:\n",
    "    print(\"Reranking BM25 Content results...\")\n",
    "    reranked_bm25_content = evaluate_reranking(\n",
    "        documents=documents_content,\n",
    "        queries=queries,\n",
    "        initial_results=bm25_content['results'],\n",
    "        retrieval_method=\"BM25\",\n",
    "        field_name=\"content\",\n",
    "        top_k=TOP_K\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    save_reranking_results(\n",
    "        reranked_bm25_content, \n",
    "        RESULTS_DIR / \"reranked_bm25_content.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping BM25 Content - results not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7768b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank BM25 metadata results\n",
    "if bm25_metadata:\n",
    "    print(\"Reranking BM25 Metadata results...\")\n",
    "    reranked_bm25_metadata = evaluate_reranking(\n",
    "        documents=documents_metadata,\n",
    "        queries=queries,\n",
    "        initial_results=bm25_metadata['results'],\n",
    "        retrieval_method=\"BM25\",\n",
    "        field_name=\"metadata\",\n",
    "        top_k=TOP_K\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    save_reranking_results(\n",
    "        reranked_bm25_metadata,\n",
    "        RESULTS_DIR / \"reranked_bm25_metadata.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping BM25 Metadata - results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa8b05",
   "metadata": {},
   "source": [
    "## 4. Rerank FAISS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank FAISS Legal-BERT content results\n",
    "if faiss_legalbert_content:\n",
    "    print(\"Reranking FAISS Legal-BERT Content results...\")\n",
    "    reranked_faiss_legalbert_content = evaluate_reranking(\n",
    "        documents=documents_content,\n",
    "        queries=queries,\n",
    "        initial_results=faiss_legalbert_content['results'],\n",
    "        retrieval_method=\"FAISS-LegalBERT\",\n",
    "        field_name=\"content\",\n",
    "        top_k=TOP_K\n",
    "    )\n",
    "    \n",
    "    save_reranking_results(\n",
    "        reranked_faiss_legalbert_content,\n",
    "        RESULTS_DIR / \"reranked_faiss_legalbert_content.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping FAISS Legal-BERT Content - results not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c22988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank FAISS BGE-Large content results\n",
    "if faiss_bge_content:\n",
    "    print(\"Reranking FAISS BGE-Large Content results...\")\n",
    "    reranked_faiss_bge_content = evaluate_reranking(\n",
    "        documents=documents_content,\n",
    "        queries=queries,\n",
    "        initial_results=faiss_bge_content['results'],\n",
    "        retrieval_method=\"FAISS-BGE\",\n",
    "        field_name=\"content\",\n",
    "        top_k=TOP_K\n",
    "    )\n",
    "    \n",
    "    save_reranking_results(\n",
    "        reranked_faiss_bge_content,\n",
    "        RESULTS_DIR / \"reranked_faiss_bge_content.json\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping FAISS BGE-Large Content - results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250754c9",
   "metadata": {},
   "source": [
    "## 5. Compare Before and After Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BM25 before and after reranking for one query\n",
    "def compare_before_after(query_id, initial_results, reranked_results, field='content'):\n",
    "    \"\"\"Compare initial and reranked results for a query.\"\"\"\n",
    "    query = queries[query_id - 1]\n",
    "    query_idx = query_id - 1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUERY {query_id}: {query['query']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Initial results\n",
    "    initial = initial_results['results'][query_idx]\n",
    "    print(\"--- INITIAL RETRIEVAL ---\")\n",
    "    print(f\"Top-3 indices: {initial['retrieved_indices'][:3]}\")\n",
    "    print(f\"Top-3 scores: {[f'{s:.4f}' for s in initial['scores'][:3]]}\\n\")\n",
    "    \n",
    "    # Reranked results\n",
    "    reranked = reranked_results['results'][query_idx]\n",
    "    print(\"--- AFTER RERANKING ---\")\n",
    "    print(f\"Top-3 indices: {reranked['reranked_indices'][:3]}\")\n",
    "    print(f\"Top-3 scores: {[f'{s:.4f}' for s in reranked['reranked_scores'][:3]]}\\n\")\n",
    "    \n",
    "    # Calculate changes\n",
    "    overlap = calculate_overlap(\n",
    "        initial['retrieved_indices'][:TOP_K],\n",
    "        reranked['reranked_indices'][:TOP_K]\n",
    "    )\n",
    "    \n",
    "    print(f\"Overlap: {overlap:.1%}\")\n",
    "    print(f\"Reranking time: {reranked['reranking_time']:.4f}s\")\n",
    "    \n",
    "    # Show if top result changed\n",
    "    if initial['retrieved_indices'][0] != reranked['reranked_indices'][0]:\n",
    "        print(\"\\n⚠️ Top result changed after reranking!\")\n",
    "        print(f\"  Before: Index {initial['retrieved_indices'][0]}\")\n",
    "        print(f\"  After:  Index {reranked['reranked_indices'][0]}\")\n",
    "\n",
    "# Example: Compare BM25 content for query 1\n",
    "if bm25_content and 'reranked_bm25_content' in locals():\n",
    "    compare_before_after(1, bm25_content, reranked_bm25_content, 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85cd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reranking impact across all queries\n",
    "if bm25_content and 'reranked_bm25_content' in locals():\n",
    "    changes_count = 0\n",
    "    top1_changes = []\n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        initial = bm25_content['results'][i]['retrieved_indices']\n",
    "        reranked = reranked_bm25_content['results'][i]['reranked_indices']\n",
    "        \n",
    "        # Check if top-1 changed\n",
    "        if initial[0] != reranked[0]:\n",
    "            changes_count += 1\n",
    "            top1_changes.append(queries[i]['id'])\n",
    "    \n",
    "    print(f\"\\nReranking Impact (BM25 Content):\")\n",
    "    print(f\"  Top-1 changed: {changes_count}/{len(queries)} queries\")\n",
    "    print(f\"  Queries with changed top-1: {top1_changes}\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    labels = ['Top-1 Same', 'Top-1 Changed']\n",
    "    sizes = [len(queries) - changes_count, changes_count]\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "    \n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Reranking Impact on Top-1 Results (BM25 Content)')\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881efb1",
   "metadata": {},
   "source": [
    "## 6. Analyze Reranking Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7659f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare reranking times across different methods\n",
    "reranking_times = []\n",
    "\n",
    "reranked_results = [\n",
    "    ('BM25 Content', reranked_bm25_content if 'reranked_bm25_content' in locals() else None),\n",
    "    ('BM25 Metadata', reranked_bm25_metadata if 'reranked_bm25_metadata' in locals() else None),\n",
    "    ('FAISS Legal-BERT', reranked_faiss_legalbert_content if 'reranked_faiss_legalbert_content' in locals() else None),\n",
    "    ('FAISS BGE', reranked_faiss_bge_content if 'reranked_faiss_bge_content' in locals() else None),\n",
    "]\n",
    "\n",
    "for name, results in reranked_results:\n",
    "    if results:\n",
    "        reranking_times.append({\n",
    "            'Method': name,\n",
    "            'Avg Reranking Time (s)': results['avg_reranking_time']\n",
    "        })\n",
    "\n",
    "if reranking_times:\n",
    "    reranking_df = pd.DataFrame(reranking_times)\n",
    "    \n",
    "    print(\"\\nReranking Performance:\")\n",
    "    print(reranking_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(reranking_df['Method'], reranking_df['Avg Reranking Time (s)'])\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Average Reranking Time (seconds)')\n",
    "    plt.title('Reranking Performance Across Methods')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a874cae",
   "metadata": {},
   "source": [
    "## 7. Detailed Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f790cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query explorer with reranking\n",
    "def explore_reranked_query(query_id):\n",
    "    \"\"\"Explore a query showing initial and reranked results.\"\"\"\n",
    "    query = queries[query_id - 1]\n",
    "    query_idx = query_id - 1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUERY {query_id}: {query['query']}\")\n",
    "    print(f\"Category: {query['category']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # BM25 Content - Before and After\n",
    "    if bm25_content and 'reranked_bm25_content' in locals():\n",
    "        print(\"\\n--- BM25 CONTENT ---\")\n",
    "        initial = bm25_content['results'][query_idx]\n",
    "        reranked = reranked_bm25_content['results'][query_idx]\n",
    "        \n",
    "        print(\"\\nBefore Reranking (Top-3):\")\n",
    "        for i in range(min(3, len(initial['retrieved_indices']))):\n",
    "            idx = initial['retrieved_indices'][i]\n",
    "            score = initial['scores'][i]\n",
    "            title = df.iloc[idx]['short_title']\n",
    "            print(f\"  {i+1}. [{score:.4f}] {title}\")\n",
    "        \n",
    "        print(\"\\nAfter Reranking (Top-3):\")\n",
    "        for i in range(min(3, len(reranked['reranked_indices']))):\n",
    "            idx = reranked['reranked_indices'][i]\n",
    "            score = reranked['reranked_scores'][i]\n",
    "            title = df.iloc[idx]['short_title']\n",
    "            marker = \"★\" if idx != initial['retrieved_indices'][i] else \" \"\n",
    "            print(f\" {marker}{i+1}. [{score:.4f}] {title}\")\n",
    "\n",
    "# Explore query 1\n",
    "explore_reranked_query(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01669e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore another query\n",
    "explore_reranked_query(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96967ce5",
   "metadata": {},
   "source": [
    "## 8. Overall Comparison: Initial vs Reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b5c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "print(\"\\nComplete Pipeline Comparison:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# BM25 Content\n",
    "if bm25_content:\n",
    "    comparison_data.append({\n",
    "        'Method': 'BM25',\n",
    "        'Field': 'Content',\n",
    "        'Stage': 'Initial',\n",
    "        'Avg Time (s)': bm25_content['avg_retrieval_time']\n",
    "    })\n",
    "    if 'reranked_bm25_content' in locals():\n",
    "        comparison_data.append({\n",
    "            'Method': 'BM25 + Reranker',\n",
    "            'Field': 'Content',\n",
    "            'Stage': 'Reranked',\n",
    "            'Avg Time (s)': bm25_content['avg_retrieval_time'] + reranked_bm25_content['avg_reranking_time']\n",
    "        })\n",
    "\n",
    "# FAISS Legal-BERT Content\n",
    "if faiss_legalbert_content:\n",
    "    comparison_data.append({\n",
    "        'Method': 'FAISS Legal-BERT',\n",
    "        'Field': 'Content',\n",
    "        'Stage': 'Initial',\n",
    "        'Avg Time (s)': faiss_legalbert_content['avg_retrieval_time']\n",
    "    })\n",
    "    if 'reranked_faiss_legalbert_content' in locals():\n",
    "        comparison_data.append({\n",
    "            'Method': 'FAISS Legal-BERT + Reranker',\n",
    "            'Field': 'Content',\n",
    "            'Stage': 'Reranked',\n",
    "            'Avg Time (s)': faiss_legalbert_content['avg_retrieval_time'] + reranked_faiss_legalbert_content['avg_reranking_time']\n",
    "        })\n",
    "\n",
    "if comparison_data:\n",
    "    final_comparison = pd.DataFrame(comparison_data)\n",
    "    print(final_comparison.to_string(index=False))\n",
    "    \n",
    "    # Save\n",
    "    final_comparison.to_csv(RESULTS_DIR / \"final_comparison_with_reranking.csv\", index=False)\n",
    "    print(f\"\\nSaved to: {RESULTS_DIR / 'final_comparison_with_reranking.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee0589",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Reranking evaluation complete!\n",
    "\n",
    "Key findings:\n",
    "- Cross-encoder reranking applied to initial retrieval results\n",
    "- Reranking can change the order of retrieved documents\n",
    "- Trade-off: better ranking quality vs increased latency\n",
    "- Two-stage retrieval (fast first-stage + reranker) is common in production\n",
    "\n",
    "Benefits of reranking:\n",
    "- ✓ More accurate relevance scoring\n",
    "- ✓ Can correct mistakes from initial retrieval\n",
    "- ✓ Cross-encoder captures query-document interactions\n",
    "\n",
    "Considerations:\n",
    "- ⚠️ Adds computational overhead\n",
    "- ⚠️ Only improves ranking, doesn't add new documents\n",
    "- ⚠️ Quality depends on initial retrieval recall\n",
    "\n",
    "Next steps:\n",
    "- Analyze which retrieval method benefits most from reranking\n",
    "- Consider hybrid approaches (BM25 + FAISS + Reranker)\n",
    "- Optimize for your latency/quality trade-off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
