{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d364770",
   "metadata": {},
   "source": [
    "# BM25 Retrieval Evaluation\n",
    "\n",
    "This notebook evaluates BM25 retrieval on:\n",
    "1. Content field\n",
    "2. Metadata field\n",
    "\n",
    "For all 10 legal queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_loader import load_data, prepare_data, get_documents_by_field\n",
    "from src.queries import get_all_queries\n",
    "from src.bm25_retriever import BM25Retriever, evaluate_bm25, save_results\n",
    "from src.evaluation import print_query_results, create_comparison_table\n",
    "from src.config import RESULTS_DIR, TOP_K\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ebd72b",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_data()\n",
    "df = prepare_data(df)\n",
    "\n",
    "print(f\"Loaded {len(df)} documents\")\n",
    "\n",
    "# Get document lists\n",
    "documents_content = get_documents_by_field(df, 'content')\n",
    "documents_metadata = get_documents_by_field(df, 'metadata')\n",
    "\n",
    "# Get queries\n",
    "queries = get_all_queries()\n",
    "print(f\"Loaded {len(queries)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dcccfc",
   "metadata": {},
   "source": [
    "## 2. BM25 Retrieval on Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20bcc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BM25 on content\n",
    "results_content = evaluate_bm25(\n",
    "    documents=documents_content,\n",
    "    queries=queries,\n",
    "    top_k=TOP_K,\n",
    "    field_name=\"content\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce665f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for first query\n",
    "query_idx = 0\n",
    "query_result = results_content['results'][query_idx]\n",
    "\n",
    "print_query_results(\n",
    "    df=df,\n",
    "    query=query_result['query'],\n",
    "    indices=query_result['retrieved_indices'],\n",
    "    scores=query_result['scores'],\n",
    "    top_n=3,\n",
    "    field='content'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8879197",
   "metadata": {},
   "source": [
    "## 3. BM25 Retrieval on Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BM25 on metadata\n",
    "results_metadata = evaluate_bm25(\n",
    "    documents=documents_metadata,\n",
    "    queries=queries,\n",
    "    top_k=TOP_K,\n",
    "    field_name=\"metadata\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for first query\n",
    "query_result = results_metadata['results'][query_idx]\n",
    "\n",
    "print_query_results(\n",
    "    df=df,\n",
    "    query=query_result['query'],\n",
    "    indices=query_result['retrieved_indices'],\n",
    "    scores=query_result['scores'],\n",
    "    top_n=3,\n",
    "    field='metadata'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d735a78",
   "metadata": {},
   "source": [
    "## 4. Compare Content vs Metadata Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af40d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    content_result = results_content['results'][i]\n",
    "    metadata_result = results_metadata['results'][i]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Query ID': query['id'],\n",
    "        'Category': query['category'],\n",
    "        'Content Time (s)': content_result['retrieval_time'],\n",
    "        'Metadata Time (s)': metadata_result['retrieval_time'],\n",
    "        'Content Top Score': content_result['scores'][0] if content_result['scores'] else 0,\n",
    "        'Metadata Top Score': metadata_result['scores'][0] if metadata_result['scores'] else 0,\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1945f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize retrieval times\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Retrieval time comparison\n",
    "axes[0].bar(range(len(queries)), comparison_df['Content Time (s)'], \n",
    "            alpha=0.7, label='Content', width=0.4, align='edge')\n",
    "axes[0].bar([x + 0.4 for x in range(len(queries))], comparison_df['Metadata Time (s)'], \n",
    "            alpha=0.7, label='Metadata', width=0.4, align='edge')\n",
    "axes[0].set_xlabel('Query ID')\n",
    "axes[0].set_ylabel('Retrieval Time (seconds)')\n",
    "axes[0].set_title('BM25 Retrieval Time Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(range(len(queries)))\n",
    "axes[0].set_xticklabels([q['id'] for q in queries])\n",
    "\n",
    "# Top score comparison\n",
    "axes[1].bar(range(len(queries)), comparison_df['Content Top Score'], \n",
    "            alpha=0.7, label='Content', width=0.4, align='edge')\n",
    "axes[1].bar([x + 0.4 for x in range(len(queries))], comparison_df['Metadata Top Score'], \n",
    "            alpha=0.7, label='Metadata', width=0.4, align='edge')\n",
    "axes[1].set_xlabel('Query ID')\n",
    "axes[1].set_ylabel('Top-1 BM25 Score')\n",
    "axes[1].set_title('BM25 Top-1 Score Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(range(len(queries)))\n",
    "axes[1].set_xticklabels([q['id'] for q in queries])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"BM25 Retrieval Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nContent Field:\")\n",
    "print(f\"  Avg retrieval time: {results_content['avg_retrieval_time']:.4f}s\")\n",
    "print(f\"  Avg top-1 score: {comparison_df['Content Top Score'].mean():.4f}\")\n",
    "print(f\"\\nMetadata Field:\")\n",
    "print(f\"  Avg retrieval time: {results_metadata['avg_retrieval_time']:.4f}s\")\n",
    "print(f\"  Avg top-1 score: {comparison_df['Metadata Top Score'].mean():.4f}\")\n",
    "print(f\"\\nSpeed comparison: Metadata is {results_content['avg_retrieval_time']/results_metadata['avg_retrieval_time']:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc21d3",
   "metadata": {},
   "source": [
    "## 5. Analyze Results for Specific Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c56307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query explorer\n",
    "def explore_query(query_id):\n",
    "    \"\"\"Display detailed results for a specific query.\"\"\"\n",
    "    query_idx = query_id - 1\n",
    "    query = queries[query_idx]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUERY {query_id}: {query['query']}\")\n",
    "    print(f\"Category: {query['category']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Content results\n",
    "    print(\"\\n--- CONTENT FIELD RESULTS ---\")\n",
    "    content_result = results_content['results'][query_idx]\n",
    "    print_query_results(\n",
    "        df, query['query'], \n",
    "        content_result['retrieved_indices'],\n",
    "        content_result['scores'],\n",
    "        top_n=3, field='content'\n",
    "    )\n",
    "    \n",
    "    # Metadata results\n",
    "    print(\"\\n--- METADATA FIELD RESULTS ---\")\n",
    "    metadata_result = results_metadata['results'][query_idx]\n",
    "    print_query_results(\n",
    "        df, query['query'],\n",
    "        metadata_result['retrieved_indices'],\n",
    "        metadata_result['scores'],\n",
    "        top_n=3, field='metadata'\n",
    "    )\n",
    "\n",
    "# Example: Explore query 1\n",
    "explore_query(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore another query of interest\n",
    "explore_query(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4981d",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17558345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both results\n",
    "save_results(results_content, RESULTS_DIR / \"bm25_content_results.json\")\n",
    "save_results(results_metadata, RESULTS_DIR / \"bm25_metadata_results.json\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(RESULTS_DIR / \"bm25_comparison.csv\", index=False)\n",
    "print(\"\\nAll results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d957a26",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "BM25 evaluation complete!\n",
    "\n",
    "Key findings:\n",
    "- BM25 retrieval tested on both content and metadata fields\n",
    "- Metadata retrieval is generally faster due to shorter documents\n",
    "- Content retrieval may capture more contextual information\n",
    "- Results saved for comparison with FAISS and reranking\n",
    "\n",
    "Next steps:\n",
    "- Build FAISS indices (notebook 03)\n",
    "- Compare with dense retrieval\n",
    "- Apply reranking (notebook 05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
