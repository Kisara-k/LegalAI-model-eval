{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c589b09e",
   "metadata": {},
   "source": [
    "# FAISS Retrieval Evaluation (CPU)\n",
    "\n",
    "This notebook evaluates FAISS retrieval using pre-built indices.\n",
    "\n",
    "**Can run on CPU** - indices are loaded from disk.\n",
    "\n",
    "Evaluates:\n",
    "- 3 embedding models (Legal-BERT, GTE-Large, BGE-Large)\n",
    "- 2 fields (content, metadata)\n",
    "- All 10 legal queries\n",
    "\n",
    "Total: 6 retrieval configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_loader import load_data, prepare_data, get_documents_by_field\n",
    "from src.queries import get_all_queries\n",
    "from src.faiss_retriever import FAISSRetriever, evaluate_faiss\n",
    "from src.evaluation import (\n",
    "    print_query_results, \n",
    "    create_comparison_table,\n",
    "    calculate_overlap,\n",
    "    analyze_retrieval_diversity\n",
    ")\n",
    "from src.config import EMBEDDING_MODELS, INDICES_DIR, RESULTS_DIR, TOP_K\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b70f7",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_data()\n",
    "df = prepare_data(df)\n",
    "\n",
    "# Get queries\n",
    "queries = get_all_queries()\n",
    "print(f\"Loaded {len(df)} documents and {len(queries)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d6099",
   "metadata": {},
   "source": [
    "## 2. Verify Indices Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a7769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which indices are available\n",
    "available_indices = []\n",
    "\n",
    "print(\"Available FAISS Indices:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_key in EMBEDDING_MODELS.keys():\n",
    "    for field in ['content', 'metadata']:\n",
    "        index_dir = INDICES_DIR / f\"{field}_{model_key}\"\n",
    "        index_file = index_dir / \"index.faiss\"\n",
    "        \n",
    "        if index_file.exists():\n",
    "            available_indices.append((model_key, field))\n",
    "            print(f\"✓ {field}_{model_key}\")\n",
    "        else:\n",
    "            print(f\"✗ {field}_{model_key} - NOT FOUND\")\n",
    "\n",
    "print(f\"\\nTotal available: {len(available_indices)} / 6\")\n",
    "\n",
    "if len(available_indices) == 0:\n",
    "    print(\"\\n⚠️ ERROR: No indices found!\")\n",
    "    print(\"Please run notebook 03 (FAISS Index Builder) first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261aedf",
   "metadata": {},
   "source": [
    "## 3. Run FAISS Retrieval for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4eae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "for model_key in EMBEDDING_MODELS.keys():\n",
    "    for field in ['content', 'metadata']:\n",
    "        index_dir = INDICES_DIR / f\"{field}_{model_key}\"\n",
    "        index_file = index_dir / \"index.faiss\"\n",
    "        \n",
    "        if not index_file.exists():\n",
    "            print(f\"Skipping {field}_{model_key} - index not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Evaluating: {model_key} on {field}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_faiss(\n",
    "            model_key=model_key,\n",
    "            queries=queries,\n",
    "            field_name=field,\n",
    "            top_k=TOP_K\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        key = f\"faiss_{model_key}_{field}\"\n",
    "        all_results[key] = results\n",
    "        \n",
    "        # Save individual results\n",
    "        output_file = RESULTS_DIR / f\"{key}_results.json\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "print(f\"\\n✓ Completed {len(all_results)} evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9bdcc8",
   "metadata": {},
   "source": [
    "## 4. Compare Models on Content Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12466caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare retrieval times for content field\n",
    "content_results = {k: v for k, v in all_results.items() if 'content' in k}\n",
    "\n",
    "if content_results:\n",
    "    comparison_data = []\n",
    "    \n",
    "    for key, results in content_results.items():\n",
    "        model = results['model']\n",
    "        comparison_data.append({\n",
    "            'Model': model,\n",
    "            'Avg Time (s)': results['avg_retrieval_time'],\n",
    "            'Model Name': results['model_name']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"Content Field - Model Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(comparison_df['Model'], comparison_df['Avg Time (s)'])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Average Retrieval Time (seconds)')\n",
    "    plt.title('FAISS Retrieval Time - Content Field')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ccbc04",
   "metadata": {},
   "source": [
    "## 5. Compare Models on Metadata Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48deac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare retrieval times for metadata field\n",
    "metadata_results = {k: v for k, v in all_results.items() if 'metadata' in k}\n",
    "\n",
    "if metadata_results:\n",
    "    comparison_data = []\n",
    "    \n",
    "    for key, results in metadata_results.items():\n",
    "        model = results['model']\n",
    "        comparison_data.append({\n",
    "            'Model': model,\n",
    "            'Avg Time (s)': results['avg_retrieval_time'],\n",
    "            'Model Name': results['model_name']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"Metadata Field - Model Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(comparison_df['Model'], comparison_df['Avg Time (s)'])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Average Retrieval Time (seconds)')\n",
    "    plt.title('FAISS Retrieval Time - Metadata Field')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3126514",
   "metadata": {},
   "source": [
    "## 6. Analyze Query-Specific Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models for a specific query\n",
    "def compare_models_for_query(query_id, field='content'):\n",
    "    \"\"\"Compare all models for a specific query.\"\"\"\n",
    "    query = queries[query_id - 1]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUERY {query_id}: {query['query']}\")\n",
    "    print(f\"Field: {field}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for model_key in EMBEDDING_MODELS.keys():\n",
    "        key = f\"faiss_{model_key}_{field}\"\n",
    "        if key in all_results:\n",
    "            results = all_results[key]\n",
    "            query_result = results['results'][query_id - 1]\n",
    "            \n",
    "            print(f\"\\n--- {model_key.upper()} ---\")\n",
    "            print_query_results(\n",
    "                df=df,\n",
    "                query=query['query'],\n",
    "                indices=query_result['retrieved_indices'],\n",
    "                scores=query_result['scores'],\n",
    "                top_n=3,\n",
    "                field=field\n",
    "            )\n",
    "\n",
    "# Example: Compare models for query 1 on content\n",
    "compare_models_for_query(1, 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54801c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare overlap between models for a query\n",
    "def analyze_model_overlap(query_id, field='content'):\n",
    "    \"\"\"Analyze overlap in results between different models.\"\"\"\n",
    "    query = queries[query_id - 1]\n",
    "    \n",
    "    print(f\"\\nQuery {query_id}: {query['query']}\")\n",
    "    print(f\"Field: {field}\\n\")\n",
    "    \n",
    "    # Get results from all models\n",
    "    model_results = {}\n",
    "    for model_key in EMBEDDING_MODELS.keys():\n",
    "        key = f\"faiss_{model_key}_{field}\"\n",
    "        if key in all_results:\n",
    "            query_result = all_results[key]['results'][query_id - 1]\n",
    "            model_results[model_key] = query_result['retrieved_indices']\n",
    "    \n",
    "    # Calculate pairwise overlap\n",
    "    models = list(model_results.keys())\n",
    "    print(\"Pairwise Overlap (Jaccard Similarity):\\n\")\n",
    "    \n",
    "    for i, model1 in enumerate(models):\n",
    "        for model2 in models[i+1:]:\n",
    "            overlap = calculate_overlap(model_results[model1], model_results[model2])\n",
    "            print(f\"{model1} vs {model2}: {overlap:.2%}\")\n",
    "\n",
    "# Example analysis\n",
    "analyze_model_overlap(1, 'content')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd691e",
   "metadata": {},
   "source": [
    "## 7. Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_table = create_comparison_table(all_results)\n",
    "\n",
    "print(\"\\nFAISS Retrieval Performance Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_table.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison_table.to_csv(RESULTS_DIR / \"faiss_comparison.csv\", index=False)\n",
    "print(f\"\\nComparison table saved to: {RESULTS_DIR / 'faiss_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d92b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all retrieval times\n",
    "if len(all_results) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    methods = []\n",
    "    times = []\n",
    "    colors = []\n",
    "    \n",
    "    for key, results in all_results.items():\n",
    "        label = f\"{results['model']}\\n({results['field']})\"\n",
    "        methods.append(label)\n",
    "        times.append(results['avg_retrieval_time'])\n",
    "        colors.append('skyblue' if results['field'] == 'content' else 'lightcoral')\n",
    "    \n",
    "    ax.barh(methods, times, color=colors)\n",
    "    ax.set_xlabel('Average Retrieval Time (seconds)')\n",
    "    ax.set_title('FAISS Retrieval Performance - All Configurations')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='skyblue', label='Content'),\n",
    "        Patch(facecolor='lightcoral', label='Metadata')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98622dc",
   "metadata": {},
   "source": [
    "## 8. Retrieval Diversity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fc31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze diversity of retrieved documents\n",
    "from src.evaluation import analyze_retrieval_diversity, print_diversity_analysis\n",
    "\n",
    "print(\"Retrieval Diversity Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for key, results in all_results.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    diversity = analyze_retrieval_diversity(results, df)\n",
    "    print_diversity_analysis(diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b6479",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "FAISS retrieval evaluation complete!\n",
    "\n",
    "Key findings:\n",
    "- Tested 3 embedding models (Legal-BERT, GTE-Large, BGE-Large)\n",
    "- Evaluated on both content and metadata fields\n",
    "- All results saved for further analysis and comparison\n",
    "- Retrieval times measured on CPU\n",
    "\n",
    "Model characteristics:\n",
    "- **Legal-BERT**: Domain-specific, may excel on legal terminology\n",
    "- **GTE-Large**: General-purpose SOTA, strong cross-domain performance  \n",
    "- **BGE-Large**: Top retrieval model, excellent for ranking tasks\n",
    "\n",
    "Next steps:\n",
    "- Compare with BM25 results (notebook 02)\n",
    "- Apply reranking to improve results (notebook 05)\n",
    "- Analyze which model works best for different query types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
