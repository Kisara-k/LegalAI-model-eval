{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e140b48",
   "metadata": {},
   "source": [
    "# FAISS Index Builder (GPU) - Google Colab Standalone\n",
    "\n",
    "**⚠️ This notebook requires a GPU environment!**\n",
    "\n",
    "## Running on Google Colab\n",
    "\n",
    "1. **Enable GPU:** Runtime → Change runtime type → Hardware accelerator: GPU (T4)\n",
    "2. **Upload dataset:** Upload `acts_with_metadata.tsv` to Colab files\n",
    "3. **Run all cells** in order\n",
    "4. **Download indices:** After completion, download the `indices.zip` file\n",
    "\n",
    "This notebook builds FAISS indices for all embedding models:\n",
    "1. Legal-BERT (`nlpaueb/legal-bert-base-uncased`)\n",
    "2. GTE-Large (`thenlper/gte-large`)\n",
    "3. BGE-Large (`BAAI/bge-large-en-v1.5`)\n",
    "4. BGE-M3 (`BAAI/bge-m3`) - 8K context window\n",
    "\n",
    "For both:\n",
    "- Content field\n",
    "- Metadata field\n",
    "\n",
    "**Total:** 8 indices (4 models × 2 fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: Google Colab already has torch and CUDA pre-installed\n",
    "\n",
    "# Check CUDA availability first\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    cuda_available = subprocess.run(['nvidia-smi'], capture_output=True).returncode == 0\n",
    "    print(f\"CUDA available: {cuda_available}\")\n",
    "except:\n",
    "    cuda_available = False\n",
    "    print(\"CUDA not available\")\n",
    "\n",
    "# Install FAISS\n",
    "# GPU version requires conda (not available via pip)\n",
    "# Google Colab doesn't have conda, so we use CPU version for now\n",
    "print(\"Installing FAISS...\")\n",
    "!pip install -q faiss-cpu  # PyPI only has CPU version\n",
    "\n",
    "# Install other packages\n",
    "print(\"Installing other dependencies...\")\n",
    "!pip install -q transformers sentence-transformers\n",
    "!pip install -q FlagEmbedding\n",
    "!pip install -q pandas numpy tqdm\n",
    "\n",
    "print(\"\\n✓ All packages installed successfully!\")\n",
    "print(\"\\n⚠️ Note: Using faiss-cpu. GPU acceleration for FAISS requires conda installation.\")\n",
    "print(\"However, embedding models (transformers) will still use GPU for encoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad91c1",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a541f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import BGE-M3 model\n",
    "try:\n",
    "    from FlagEmbedding import BGEM3FlagModel\n",
    "    BGE_M3_AVAILABLE = True\n",
    "except ImportError:\n",
    "    BGE_M3_AVAILABLE = False\n",
    "    print(\"Warning: FlagEmbedding not available. BGE-M3 model will not work.\")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISSIndexBuilder:\n",
    "    \"\"\"FAISS index builder for embedding models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"Initialize the index builder with a specific model.\"\"\"\n",
    "        self.model_key = model_name\n",
    "        self.model_config = EMBEDDING_MODELS[model_name]\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.library = self.model_config.get(\"library\", \"sentence-transformers\")\n",
    "        \n",
    "        print(f\"Loading model: {self.model_config['name']}\")\n",
    "        print(f\"Library: {self.library}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        \n",
    "        # Load embedding model\n",
    "        if self.library == \"flagembedding\":\n",
    "            if not BGE_M3_AVAILABLE:\n",
    "                raise ImportError(\"FlagEmbedding library required for BGE-M3\")\n",
    "            self.model = BGEM3FlagModel(\n",
    "                self.model_config[\"name\"],\n",
    "                use_fp16=True\n",
    "            )\n",
    "        else:\n",
    "            self.model = SentenceTransformer(\n",
    "                self.model_config[\"name\"],\n",
    "                device=self.device\n",
    "            )\n",
    "        \n",
    "        print(\"✓ Model loaded successfully\")\n",
    "    \n",
    "    def encode_documents(\n",
    "        self, \n",
    "        documents: List[str],\n",
    "        batch_size: int = 128,\n",
    "        show_progress: bool = True\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Encode documents into embeddings.\"\"\"\n",
    "        print(f\"Encoding {len(documents)} documents...\")\n",
    "        \n",
    "        if self.library == \"flagembedding\":\n",
    "            # BGE-M3 encoding\n",
    "            max_length = self.model_config.get(\"max_length\", 8192)\n",
    "            embeddings = self.model.encode(\n",
    "                documents,\n",
    "                batch_size=batch_size,\n",
    "                max_length=max_length,\n",
    "                return_dense=True,\n",
    "                return_sparse=False,\n",
    "                return_colbert_vecs=False\n",
    "            )['dense_vecs']\n",
    "            # Normalize\n",
    "            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        else:\n",
    "            # SentenceTransformer encoding\n",
    "            embeddings = self.model.encode(\n",
    "                documents,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=show_progress,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True,\n",
    "            )\n",
    "        \n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def build_index(\n",
    "        self, \n",
    "        documents: List[str],\n",
    "        batch_size: int = 128,\n",
    "        use_gpu: bool = False\n",
    "    ):\n",
    "        \"\"\"Build FAISS index from documents.\"\"\"\n",
    "        print(f\"\\nBuilding FAISS index for {self.model_key}...\")\n",
    "        \n",
    "        # Encode documents (this uses GPU via PyTorch/Transformers)\n",
    "        embeddings = self.encode_documents(documents, batch_size)\n",
    "        \n",
    "        # Create FAISS index (CPU-only with faiss-cpu package)\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatIP(dimension)  # Inner product (for normalized vectors = cosine)\n",
    "        \n",
    "        # Check if FAISS has GPU support\n",
    "        has_gpu_support = hasattr(faiss, 'StandardGpuResources')\n",
    "        \n",
    "        if use_gpu and has_gpu_support and torch.cuda.is_available():\n",
    "            # Only use GPU FAISS if available (requires faiss-gpu package)\n",
    "            print(\"Moving FAISS index to GPU...\")\n",
    "            res = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "        else:\n",
    "            if use_gpu and not has_gpu_support:\n",
    "                print(\"⚠️ FAISS GPU not available (using faiss-cpu). Index operations will use CPU.\")\n",
    "                print(\"   Note: Embeddings are still computed on GPU (the slow part)!\")\n",
    "        \n",
    "        # Add embeddings\n",
    "        print(\"Adding embeddings to index...\")\n",
    "        index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"✓ Index built: {index.ntotal} vectors\")\n",
    "        \n",
    "        # Convert back to CPU for saving if needed\n",
    "        if use_gpu and has_gpu_support and torch.cuda.is_available():\n",
    "            index = faiss.index_gpu_to_cpu(index)\n",
    "        \n",
    "        self.index = index\n",
    "        self.documents = documents\n",
    "        \n",
    "        return index\n",
    "    \n",
    "    def save_index(self, save_dir: Path):\n",
    "        \"\"\"Save FAISS index and metadata.\"\"\"\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        index_path = save_dir / \"index.faiss\"\n",
    "        faiss.write_index(self.index, str(index_path))\n",
    "        print(f\"✓ Index saved to: {index_path}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            \"model_name\": self.model_config[\"name\"],\n",
    "            \"model_key\": self.model_key,\n",
    "            \"num_documents\": len(self.documents),\n",
    "            \"dimension\": self.index.d,\n",
    "            \"library\": self.library,\n",
    "        }\n",
    "        \n",
    "        metadata_path = save_dir / \"metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        print(f\"✓ Metadata saved to: {metadata_path}\")\n",
    "        \n",
    "        # Save document list (for reference)\n",
    "        docs_path = save_dir / \"documents.txt\"\n",
    "        with open(docs_path, 'w', encoding='utf-8') as f:\n",
    "            for doc in self.documents:\n",
    "                f.write(doc + '\\n')\n",
    "        print(f\"✓ Documents list saved to: {docs_path}\")\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3877fec",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e97e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBEDDING_MODELS = {\n",
    "    \"legal-bert\": {\n",
    "        \"name\": \"nlpaueb/legal-bert-base-uncased\",\n",
    "        \"description\": \"Domain-specific BERT model trained on legal corpora\",\n",
    "        \"max_length\": 512,\n",
    "        \"library\": \"sentence-transformers\",\n",
    "    },\n",
    "    \"gte-large\": {\n",
    "        \"name\": \"thenlper/gte-large\",\n",
    "        \"description\": \"State-of-the-art general-purpose embedding model\",\n",
    "        \"max_length\": 512,\n",
    "        \"library\": \"sentence-transformers\",\n",
    "    },\n",
    "    \"bge-large\": {\n",
    "        \"name\": \"BAAI/bge-large-en-v1.5\",\n",
    "        \"description\": \"Top-performing model for retrieval tasks\",\n",
    "        \"max_length\": 512,\n",
    "        \"library\": \"sentence-transformers\",\n",
    "    },\n",
    "    \"bge-m3\": {\n",
    "        \"name\": \"BAAI/bge-m3\",\n",
    "        \"description\": \"BGE-M3: Multi-Functionality, Multi-Linguality model with 8192 token support\",\n",
    "        \"max_length\": 8192,\n",
    "        \"library\": \"flagembedding\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Batch size for encoding (adjust based on your GPU VRAM)\n",
    "# 128 for 16GB GPU, 256 for 24GB GPU, 64 for 12GB GPU\n",
    "EMBEDDING_BATCH_SIZE = 128\n",
    "\n",
    "# Metadata fields to combine\n",
    "METADATA_FIELDS = [\"short_title\", \"keywords\", \"section_title\", \"summary\"]\n",
    "METADATA_SEPARATOR = \" | \"\n",
    "\n",
    "# Output directory for indices\n",
    "INDICES_DIR = Path(\"indices\")\n",
    "INDICES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✓ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184617ed",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04c4d6",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\n⚠️ WARNING: No GPU detected!\")\n",
    "    print(\"This notebook is designed for GPU. Index building will be slow on CPU.\")\n",
    "    print(\"Consider running on a GPU-enabled environment (Google Colab, Kaggle, etc.)\")\n",
    "\n",
    "# Check FAISS capabilities\n",
    "print(f\"\\nFAISS version: {faiss.__version__}\")\n",
    "has_gpu = hasattr(faiss, 'StandardGpuResources')\n",
    "print(f\"FAISS GPU support: {has_gpu}\")\n",
    "\n",
    "if not has_gpu:\n",
    "    print(\"\\n💡 Using faiss-cpu package (expected for Colab)\")\n",
    "    print(\"   ✅ GPU will still be used for encoding (95% of compute time)\")\n",
    "    print(\"   ✅ FAISS index operations on CPU are fast anyway (~2-3 min per model)\")\n",
    "    print(\"   ⏱️ Total impact: ~5 minutes extra across all models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbed2e",
   "metadata": {},
   "source": [
    "## 2. Upload and Load Dataset\n",
    "\n",
    "**⚠️ Upload your `acts_with_metadata.tsv` file using the file upload button in Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload file manually in Colab\n",
    "# Click the folder icon on the left, then upload acts_with_metadata.tsv\n",
    "\n",
    "# Option 2: If you have it in Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# data_file = '/content/drive/MyDrive/path/to/acts_with_metadata.tsv'\n",
    "\n",
    "# For local Colab upload:\n",
    "data_file = 'acts_with_metadata.tsv'\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(data_file, sep='\\t')\n",
    "\n",
    "print(f\"✓ Loaded {len(df)} documents\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47780005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metadata field (join 4 fields into 1)\n",
    "print(\"Preparing metadata field...\")\n",
    "\n",
    "# Fill NaN values\n",
    "for field in METADATA_FIELDS:\n",
    "    if field in df.columns:\n",
    "        df[field] = df[field].fillna('')\n",
    "\n",
    "# Join metadata fields\n",
    "df['metadata'] = df.apply(\n",
    "    lambda row: METADATA_SEPARATOR.join([\n",
    "        str(row.get(field, '')) for field in METADATA_FIELDS\n",
    "    ]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"✓ Metadata field created\")\n",
    "\n",
    "# Get document lists\n",
    "documents_content = df['content'].astype(str).tolist()\n",
    "documents_metadata = df['metadata'].astype(str).tolist()\n",
    "\n",
    "print(f\"\\n✓ Content documents: {len(documents_content)}\")\n",
    "print(f\"✓ Metadata documents: {len(documents_metadata)}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nContent example:\")\n",
    "print(documents_content[0][:200] + \"...\")\n",
    "print(\"\\nMetadata example:\")\n",
    "print(documents_metadata[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e154cdc0",
   "metadata": {},
   "source": [
    "## Prepare Data Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee64f5",
   "metadata": {},
   "source": [
    "## 3. Review Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42853ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display embedding models configuration\n",
    "print(\"Embedding Models Configuration:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for key, config in EMBEDDING_MODELS.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Model: {config['name']}\")\n",
    "    print(f\"  Description: {config['description']}\")\n",
    "    print(f\"  Max Length: {config['max_length']} tokens\")\n",
    "    print(f\"  Library: {config['library']}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Batch Size: {EMBEDDING_BATCH_SIZE}\")\n",
    "print(f\"Total Models: {len(EMBEDDING_MODELS)}\")\n",
    "print(f\"Total Indices to Build: {len(EMBEDDING_MODELS) * 2} (content + metadata for each)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e2717",
   "metadata": {},
   "source": [
    "## 4. Build All FAISS Indices\n",
    "\n",
    "This will build 8 indices total:\n",
    "- 4 models × 2 fields (content + metadata)\n",
    "\n",
    "**Models:**\n",
    "1. Legal-BERT (512 tokens, 768-dim)\n",
    "2. GTE-Large (512 tokens, 1024-dim)\n",
    "3. BGE-Large (512 tokens, 1024-dim)\n",
    "4. BGE-M3 (8192 tokens, 1024-dim) - Long context!\n",
    "\n",
    "**Note:** This may take 40-60 minutes depending on GPU. Monitor GPU usage with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build all indices\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(f\"Building indices with GPU: {use_gpu}\")\n",
    "print(f\"Batch size: {EMBEDDING_BATCH_SIZE}\")\n",
    "print(f\"\\nThis will take some time...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build indices for each model\n",
    "for model_idx, model_key in enumerate(EMBEDDING_MODELS.keys(), 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL {model_idx}/{len(EMBEDDING_MODELS)}: {model_key}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize builder\n",
    "        builder = FAISSIndexBuilder(model_key)\n",
    "        \n",
    "        # Build content index\n",
    "        print(f\"\\n--- Building CONTENT index for {model_key} ---\")\n",
    "        builder.build_index(documents_content, batch_size=EMBEDDING_BATCH_SIZE, use_gpu=use_gpu)\n",
    "        content_dir = INDICES_DIR / f\"content_{model_key}\"\n",
    "        builder.save_index(content_dir)\n",
    "        \n",
    "        # Build metadata index\n",
    "        print(f\"\\n--- Building METADATA index for {model_key} ---\")\n",
    "        builder = FAISSIndexBuilder(model_key)  # Reinitialize for clean state\n",
    "        builder.build_index(documents_metadata, batch_size=EMBEDDING_BATCH_SIZE, use_gpu=use_gpu)\n",
    "        metadata_dir = INDICES_DIR / f\"metadata_{model_key}\"\n",
    "        builder.save_index(metadata_dir)\n",
    "        \n",
    "        print(f\"\\n✓ Completed {model_key} ({model_idx}/{len(EMBEDDING_MODELS)})\")\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error building indices for {model_key}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ ALL INDICES BUILT AND SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU Memory Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    allocated_pct = (torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory) * 100\n",
    "    print(f\"\\nGPU Utilization: {allocated_pct:.1f}%\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2f0668",
   "metadata": {},
   "source": [
    "## (Optional) Monitor GPU Usage\n",
    "\n",
    "Run this in a separate cell while building indices to monitor GPU utilization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd9866",
   "metadata": {},
   "source": [
    "## 5. Verify Saved Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa5741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved indices\n",
    "import json\n",
    "\n",
    "print(\"Saved FAISS Indices:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_key in EMBEDDING_MODELS.keys():\n",
    "    for field in ['content', 'metadata']:\n",
    "        index_dir = INDICES_DIR / f\"{field}_{model_key}\"\n",
    "        \n",
    "        if index_dir.exists():\n",
    "            # Load metadata\n",
    "            metadata_file = index_dir / \"metadata.json\"\n",
    "            if metadata_file.exists():\n",
    "                with open(metadata_file, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                \n",
    "                print(f\"\\n{field}_{model_key}:\")\n",
    "                print(f\"  Model: {metadata['model_name']}\")\n",
    "                print(f\"  Documents: {metadata['num_documents']}\")\n",
    "                print(f\"  Dimension: {metadata['dimension']}\")\n",
    "                \n",
    "                # Check file sizes\n",
    "                index_file = index_dir / \"index.faiss\"\n",
    "                if index_file.exists():\n",
    "                    size_mb = index_file.stat().st_size / (1024 * 1024)\n",
    "                    print(f\"  Index size: {size_mb:.2f} MB\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️ {field}_{model_key}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439ec3c",
   "metadata": {},
   "source": [
    "## 6. Download Indices (for use on local machine)\n",
    "\n",
    "Zip all indices and download to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of all indices\n",
    "import shutil\n",
    "\n",
    "print(\"Creating zip archive of indices...\")\n",
    "\n",
    "# Create zip file\n",
    "zip_path = shutil.make_archive('indices', 'zip', INDICES_DIR)\n",
    "\n",
    "print(f\"✓ Indices zipped to: {zip_path}\")\n",
    "\n",
    "# Get file size\n",
    "import os\n",
    "zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
    "print(f\"Archive size: {zip_size_mb:.2f} MB\")\n",
    "\n",
    "# In Colab, download the file\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\nDownloading zip file...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"✓ Download started!\")\n",
    "except ImportError:\n",
    "    print(\"\\nNot running in Colab - file saved locally as 'indices.zip'\")\n",
    "    print(\"You can download it manually from the files panel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5054691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify indices work\n",
    "test_model = \"legal-bert\"\n",
    "test_field = \"content\"\n",
    "\n",
    "print(f\"Testing index: {test_field}_{test_model}\\n\")\n",
    "\n",
    "# Load index\n",
    "index_path = INDICES_DIR / f\"{test_field}_{test_model}\" / \"index.faiss\"\n",
    "index = faiss.read_index(str(index_path))\n",
    "\n",
    "# Load model for query encoding\n",
    "builder = FAISSIndexBuilder(test_model)\n",
    "\n",
    "# Test query\n",
    "test_query = \"What are the procedures for presidential elections?\"\n",
    "print(f\"Test query: {test_query}\\n\")\n",
    "\n",
    "# Encode query\n",
    "query_embedding = builder.encode_documents([test_query], batch_size=1, show_progress=False)\n",
    "\n",
    "# Search\n",
    "k = 5\n",
    "scores, indices = index.search(query_embedding.astype('float32'), k)\n",
    "\n",
    "print(f\"Top {k} results:\")\n",
    "for i, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "    print(f\"\\n{i+1}. Document Index: {idx}\")\n",
    "    print(f\"   Score: {score:.4f}\")\n",
    "    print(f\"   Content: {documents_content[idx][:150]}...\")\n",
    "\n",
    "print(\"\\n✓ Index loading and search working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea7c9b",
   "metadata": {},
   "source": [
    "## 7. Test Index Loading (Verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07d2e4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ✅ FAISS Index Building Complete!\n",
    "\n",
    "Created **8 indices** for:\n",
    "\n",
    "1. **Legal-BERT** (content + metadata)\n",
    "   - Domain-specific legal model\n",
    "   - 768-dim embeddings, 512 max tokens\n",
    "\n",
    "2. **GTE-Large** (content + metadata)\n",
    "   - State-of-the-art general embedding\n",
    "   - 1024-dim embeddings, 512 max tokens\n",
    "\n",
    "3. **BGE-Large** (content + metadata)\n",
    "   - Top MTEB leaderboard model\n",
    "   - 1024-dim embeddings, 512 max tokens\n",
    "\n",
    "4. **BGE-M3** (content + metadata) 🆕\n",
    "   - Multi-lingual, long-context model\n",
    "   - 1024-dim embeddings, **8192 max tokens**\n",
    "   - Supports 100+ languages\n",
    "\n",
    "All indices saved to: `indices/` directory\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download** the `indices.zip` file to your local machine\n",
    "2. **Extract** to your project's `indices/` folder\n",
    "3. **Run** the FAISS retrieval notebook (03) on CPU\n",
    "4. **Compare** with BM25 results\n",
    "5. **Apply** reranking (notebook 04)\n",
    "\n",
    "### File Structure:\n",
    "```\n",
    "indices/\n",
    "├── content_legal-bert/\n",
    "│   ├── index.faiss\n",
    "│   ├── metadata.json\n",
    "│   └── documents.txt\n",
    "├── content_gte-large/\n",
    "├── content_bge-large/\n",
    "├── content_bge-m3/\n",
    "├── metadata_legal-bert/\n",
    "├── metadata_gte-large/\n",
    "├── metadata_bge-large/\n",
    "└── metadata_bge-m3/\n",
    "```\n",
    "\n",
    "**Total Size:** ~22 GB (8 indices)\n",
    "\n",
    "---\n",
    "\n",
    "**💡 Tip:** You can now run retrieval and evaluation on CPU. Only index building requires GPU!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
